#!/usr/bin/env python3
"""
üöÄ SCRIPT MAESTRO - AI ENGINE COMPLETO
=====================================
Script que actualiza/crea TODOS los archivos necesarios
para que funcione el selector de modelos con BERT, Llama, R1, APIs
"""

import os
import sys
from pathlib import Path
import subprocess

class AIEngineCompleteSetup:
    def __init__(self):
        self.ai_engine_path = Path("services/ai-engine")
        self.files_to_create = []
        
    def run(self):
        """Ejecutar setup completo"""
        print("üöÄ SCRIPT MAESTRO - AI ENGINE COMPLETO")
        print("=" * 50)
        
        try:
            # 1. Verificar estructura existente
            self.verify_structure()
            
            # 2. Actualizar app.py principal
            self.update_main_app()
            
            # 3. Actualizar hybrid engine
            self.update_hybrid_engine()
            
            # 4. Actualizar routers
            self.update_routers()
            
            # 5. Actualizar model manager
            self.update_model_manager()
            
            # 6. Crear archivos de configuraci√≥n
            self.create_config_files()
            
            # 7. Actualizar requirements
            self.update_requirements()
            
            # 8. Verificar funcionamiento
            self.verify_functionality()
            
            print("\n‚úÖ AI ENGINE COMPLETADO AL 100%")
            print("üîß Para ejecutar: cd services/ai-engine && python app.py")
            print("üåê URL: http://localhost:8001")
            
        except Exception as e:
            print(f"‚ùå Error: {e}")
            return False
            
        return True
    
    def verify_structure(self):
        """Verificar estructura existente"""
        print("üìÅ Verificando estructura existente...")
        
        required_dirs = [
            "models", "routers", "services", "config", "utils", "logs", "data", "cache"
        ]
        
        for dir_name in required_dirs:
            dir_path = self.ai_engine_path / dir_name
            if not dir_path.exists():
                dir_path.mkdir(parents=True, exist_ok=True)
                (dir_path / "__init__.py").touch()
                print(f"üìÅ Creado: {dir_name}/")
        
        print("‚úÖ Estructura verificada")
    
    def update_main_app(self):
        """Actualizar app.py principal"""
        print("üîß Actualizando app.py principal...")
        
        app_content = '''#!/usr/bin/env python3
"""
ü§ñ AI ENGINE v6.0 - APLICACI√ìN PRINCIPAL
=======================================
Microservicio de IA con selector de modelos
Puerto: 8001
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import uvicorn
import asyncio
import logging
from contextlib import asynccontextmanager
from datetime import datetime
from typing import Dict, List, Optional, Any

# Importar m√≥dulos locales
from models.llm_hybrid_engine import HybridLLMEngine
from services.model_manager import ModelManager
from routers import chat, models as models_router, analysis, generation, training

# Configuraci√≥n logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Variables globales
hybrid_engine = None
model_manager = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Ciclo de vida de la aplicaci√≥n"""
    global hybrid_engine, model_manager
    
    logger.info("üöÄ Iniciando AI-Engine...")
    
    try:
        # Inicializar gestores
        model_manager = ModelManager()
        hybrid_engine = HybridLLMEngine()
        
        # Inicializaci√≥n as√≠ncrona
        await hybrid_engine.initialize()
        await model_manager.initialize()
        
        logger.info("‚úÖ AI-Engine iniciado correctamente")
        
    except Exception as e:
        logger.error(f"‚ùå Error iniciando AI-Engine: {e}")
        # Continuar con modo b√°sico
        pass
    
    yield
    
    # Shutdown
    logger.info("üîÑ Cerrando AI-Engine...")
    if hybrid_engine:
        await hybrid_engine.cleanup()

# Crear aplicaci√≥n FastAPI
app = FastAPI(
    title="ü§ñ AI-Engine - Agente IA OyP 6.0",
    description="Microservicio de IA con modelos locales y cloud",
    version="6.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan
)

# Configurar CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# =====================
# ENDPOINTS PRINCIPALES
# =====================

@app.get("/health")
async def health_check():
    """Health check del servicio"""
    return {
        "status": "healthy",
        "service": "ai-engine",
        "version": "6.0.0",
        "timestamp": datetime.now().isoformat(),
        "models_ready": hybrid_engine.is_ready() if hybrid_engine else False
    }

@app.get("/status")
async def detailed_status():
    """Estado detallado del servicio"""
    try:
        if not hybrid_engine:
            return {"status": "initializing", "models": {"available": 0}}
        
        available_models = await hybrid_engine.get_available_models()
        stats = await hybrid_engine.get_stats()
        
        return {
            "status": "operational",
            "service": "ai-engine",
            "version": "6.0.0",
            "models": available_models,
            "stats": stats,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.get("/models")
async def get_models():
    """Obtener modelos disponibles para el selector"""
    try:
        if not hybrid_engine:
            return {"models": [], "default_model": "none"}
        
        models = await hybrid_engine.get_available_models()
        default_model = await hybrid_engine.get_default_model()
        
        # Formatear para el frontend
        formatted_models = []
        for model_info in models.get("all", []):
            formatted_models.append({
                "id": model_info["id"],
                "name": model_info["name"],
                "type": model_info["type"],
                "status": model_info["status"],
                "description": model_info["description"],
                "icon": model_info["icon"],
                "provider": model_info["provider"],
                "requires_api_key": model_info.get("requires_api_key", False),
                "is_default": model_info["id"] == default_model
            })
        
        return {
            "models": formatted_models,
            "default_model": default_model,
            "total_models": len(formatted_models)
        }
        
    except Exception as e:
        logger.error(f"Error obteniendo modelos: {e}")
        return {"models": [], "default_model": "none", "error": str(e)}

@app.post("/chat")
async def chat_with_ai(request: dict):
    """Chat con IA usando modelo seleccionado"""
    try:
        if not hybrid_engine:
            raise HTTPException(status_code=503, detail="AI Engine no disponible")
        
        message = request.get("message", "")
        model_id = request.get("model", None)
        history = request.get("history", [])
        
        if not message:
            raise HTTPException(status_code=400, detail="Mensaje requerido")
        
        # Generar respuesta
        response = await hybrid_engine.chat(
            message=message,
            model_id=model_id,
            conversation_history=history
        )
        
        return response
        
    except Exception as e:
        logger.error(f"Error en chat: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/configure")
async def configure_model(request: dict):
    """Configurar API key para modelo cloud"""
    try:
        if not hybrid_engine:
            raise HTTPException(status_code=503, detail="AI Engine no disponible")
        
        model_id = request.get("model_id")
        api_key = request.get("api_key")
        
        if not model_id or not api_key:
            raise HTTPException(status_code=400, detail="model_id y api_key requeridos")
        
        success = await hybrid_engine.set_api_key(model_id, api_key)
        
        if success:
            return {"status": "success", "message": f"API key configurada para {model_id}"}
        else:
            return {"status": "error", "message": f"Error configurando {model_id}"}
            
    except Exception as e:
        logger.error(f"Error configurando modelo: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/test-model")
async def test_model(request: dict):
    """Probar modelo espec√≠fico"""
    try:
        if not hybrid_engine:
            raise HTTPException(status_code=503, detail="AI Engine no disponible")
        
        model_id = request.get("model_id")
        test_message = request.get("message", "Hola, ¬øfuncionas correctamente?")
        
        if not model_id:
            raise HTTPException(status_code=400, detail="model_id requerido")
        
        # Probar modelo
        response = await hybrid_engine.chat(
            message=test_message,
            model_id=model_id,
            conversation_history=[]
        )
        
        if "error" in response:
            return {"status": "error", "message": response["error"]}
        else:
            return {"status": "success", "response": response}
            
    except Exception as e:
        logger.error(f"Error probando modelo: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Incluir routers
app.include_router(chat.router, prefix="/chat", tags=["Chat"])
app.include_router(models_router.router, prefix="/models", tags=["Models"])
app.include_router(analysis.router, prefix="/analyze", tags=["Analysis"])
app.include_router(generation.router, prefix="/generate", tags=["Generation"])
app.include_router(training.router, prefix="/train", tags=["Training"])

# =====================
# FUNCIONES HELPER
# =====================

def get_hybrid_engine():
    """Dependency para obtener hybrid engine"""
    if not hybrid_engine:
        raise HTTPException(status_code=503, detail="AI Engine no disponible")
    return hybrid_engine

def get_model_manager():
    """Dependency para obtener model manager"""
    if not model_manager:
        raise HTTPException(status_code=503, detail="Model Manager no disponible")
    return model_manager

# =====================
# MAIN
# =====================

if __name__ == "__main__":
    logger.info("üöÄ Iniciando AI-Engine en modo desarrollo...")
    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=8001,
        reload=True,
        log_level="info"
    )
'''
        
        with open(self.ai_engine_path / "app.py", "w", encoding="utf-8") as f:
            f.write(app_content)
        
        print("‚úÖ app.py actualizado")
    
    def update_hybrid_engine(self):
        """Actualizar hybrid engine completo"""
        print("ü§ñ Actualizando Hybrid Engine...")
        
        hybrid_engine_content = '''#!/usr/bin/env python3
"""
ü§ñ HYBRID LLM ENGINE v6.0
========================
Motor h√≠brido completo con todos los modelos
"""

import asyncio
import logging
import os
import json
from typing import Dict, List, Optional, Any
from datetime import datetime
from enum import Enum

# Imports opcionales para manejar dependencias
try:
    from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, Conversation
    import torch
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

try:
    import google.generativeai as genai
    GOOGLE_AI_AVAILABLE = True
except ImportError:
    GOOGLE_AI_AVAILABLE = False

try:
    import httpx
    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False

logger = logging.getLogger(__name__)

class ModelType(str, Enum):
    LOCAL = "local"
    CLOUD = "cloud"

class ModelStatus(str, Enum):
    READY = "ready"
    LOADING = "loading"
    API_KEY_REQUIRED = "api_key_required"
    ERROR = "error"
    NOT_AVAILABLE = "not_available"

class HybridLLMEngine:
    """Motor h√≠brido de LLM con auto-detecci√≥n de modelos"""
    
    def __init__(self):
        self.is_initialized = False
        self.loaded_models = {}
        self.api_keys = {}
        self.default_model = "bert"
        
        # Configuraci√≥n de modelos disponibles
        self.model_configs = {
            "bert": {
                "id": "bert",
                "name": "BERT Conversational",
                "type": ModelType.LOCAL,
                "status": ModelStatus.NOT_AVAILABLE,
                "description": "Modelo BERT local para conversaci√≥n",
                "icon": "ü§ñ",
                "provider": "Hugging Face",
                "requires_api_key": False,
                "model_name": "microsoft/DialoGPT-medium"
            },
            "llama": {
                "id": "llama",
                "name": "Llama 3.1",
                "type": ModelType.LOCAL,
                "status": ModelStatus.NOT_AVAILABLE,
                "description": "Llama 3.1 via Ollama",
                "icon": "ü¶ô",
                "provider": "Meta",
                "requires_api_key": False,
                "endpoint": "http://localhost:11434",
                "model_name": "llama3.1"
            },
            "r1": {
                "id": "r1",
                "name": "R1 DeepSeek",
                "type": ModelType.LOCAL,
                "status": ModelStatus.NOT_AVAILABLE,
                "description": "DeepSeek R1 via Ollama",
                "icon": "üß†",
                "provider": "DeepSeek",
                "requires_api_key": False,
                "endpoint": "http://localhost:11434",
                "model_name": "deepseek-r1"
            },
            "gpt4": {
                "id": "gpt4",
                "name": "GPT-4",
                "type": ModelType.CLOUD,
                "status": ModelStatus.API_KEY_REQUIRED,
                "description": "GPT-4 de OpenAI",
                "icon": "üí°",
                "provider": "OpenAI",
                "requires_api_key": True,
                "api_key_env": "OPENAI_API_KEY"
            },
            "claude": {
                "id": "claude",
                "name": "Claude 3.5",
                "type": ModelType.CLOUD,
                "status": ModelStatus.API_KEY_REQUIRED,
                "description": "Claude 3.5 Sonnet",
                "icon": "üß†",
                "provider": "Anthropic",
                "requires_api_key": True,
                "api_key_env": "ANTHROPIC_API_KEY"
            },
            "gemini": {
                "id": "gemini",
                "name": "Gemini Pro",
                "type": ModelType.CLOUD,
                "status": ModelStatus.API_KEY_REQUIRED,
                "description": "Google Gemini Pro",
                "icon": "‚≠ê",
                "provider": "Google",
                "requires_api_key": True,
                "api_key_env": "GOOGLE_AI_API_KEY"
            }
        }
        
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "models_loaded": 0,
            "last_request": None
        }
    
    async def initialize(self):
        """Inicializar el motor h√≠brido"""
        logger.info("ü§ñ Inicializando HybridLLMEngine...")
        
        try:
            # Detectar modelos locales
            await self._detect_local_models()
            
            # Verificar APIs cloud
            await self._check_cloud_apis()
            
            # Configurar modelo por defecto
            self._set_default_model()
            
            self.is_initialized = True
            logger.info("‚úÖ HybridLLMEngine inicializado correctamente")
            
        except Exception as e:
            logger.error(f"‚ùå Error inicializando HybridLLMEngine: {e}")
            # Continuar en modo b√°sico
            self.is_initialized = True
    
    async def _detect_local_models(self):
        """Detectar autom√°ticamente modelos locales"""
        
        # 1. Detectar BERT
        if TRANSFORMERS_AVAILABLE:
            try:
                logger.info("ü§ñ Detectando BERT...")
                self.model_configs["bert"]["status"] = ModelStatus.LOADING
                
                # Intentar cargar BERT
                chatbot = pipeline(
                    "conversational",
                    model="microsoft/DialoGPT-medium",
                    tokenizer="microsoft/DialoGPT-medium"
                )
                
                self.loaded_models["bert"] = chatbot
                self.model_configs["bert"]["status"] = ModelStatus.READY
                self.stats["models_loaded"] += 1
                logger.info("‚úÖ BERT cargado correctamente")
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error cargando BERT: {e}")
                self.model_configs["bert"]["status"] = ModelStatus.ERROR
        else:
            logger.warning("‚ö†Ô∏è Transformers no disponible para BERT")
        
        # 2. Detectar Ollama models
        await self._check_ollama_model("llama", "llama3.1")
        await self._check_ollama_model("r1", "deepseek-r1")
    
    async def _check_ollama_model(self, model_id: str, ollama_name: str):
        """Verificar modelo en Ollama"""
        if not HTTPX_AVAILABLE:
            logger.warning(f"‚ö†Ô∏è httpx no disponible para verificar {model_id}")
            return
        
        try:
            logger.info(f"üîç Verificando {model_id} en Ollama...")
            
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get("http://localhost:11434/api/tags")
                
                if response.status_code == 200:
                    models = response.json().get("models", [])
                    model_names = [m.get("name", "").split(":")[0] for m in models]
                    
                    if ollama_name in model_names:
                        self.model_configs[model_id]["status"] = ModelStatus.READY
                        self.stats["models_loaded"] += 1
                        logger.info(f"‚úÖ {model_id} disponible en Ollama")
                    else:
                        self.model_configs[model_id]["status"] = ModelStatus.NOT_AVAILABLE
                        logger.warning(f"‚ö†Ô∏è {model_id} no encontrado en Ollama")
                        
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error verificando {model_id}: {e}")
            self.model_configs[model_id]["status"] = ModelStatus.NOT_AVAILABLE
    
    async def _check_cloud_apis(self):
        """Verificar estado de APIs cloud"""
        
        # Cargar API keys desde variables de entorno
        for model_id, config in self.model_configs.items():
            if config.get("requires_api_key") and config.get("api_key_env"):
                api_key = os.getenv(config["api_key_env"])
                if api_key:
                    self.api_keys[model_id] = api_key
                    config["status"] = ModelStatus.READY
                    self.stats["models_loaded"] += 1
                    logger.info(f"‚úÖ API key encontrada para {model_id}")
    
    def _set_default_model(self):
        """Configurar modelo por defecto"""
        priority = ["bert", "llama", "r1", "gpt4", "claude", "gemini"]
        
        for model_id in priority:
            if self.model_configs[model_id]["status"] == ModelStatus.READY:
                self.default_model = model_id
                logger.info(f"üéØ Modelo por defecto: {model_id}")
                return
        
        logger.warning("‚ö†Ô∏è Ning√∫n modelo disponible por defecto")
    
    def is_ready(self) -> bool:
        """Verificar si el motor est√° listo"""
        return self.is_initialized
    
    async def get_available_models(self) -> Dict[str, Any]:
        """Obtener modelos disponibles"""
        
        all_models = []
        local_models = []
        cloud_models = []
        
        for model_id, config in self.model_configs.items():
            model_info = {
                "id": config["id"],
                "name": config["name"],
                "type": config["type"].value,
                "status": config["status"].value,
                "description": config["description"],
                "icon": config["icon"],
                "provider": config["provider"],
                "requires_api_key": config.get("requires_api_key", False)
            }
            
            all_models.append(model_info)
            
            if config["type"] == ModelType.LOCAL:
                local_models.append(model_info)
            else:
                cloud_models.append(model_info)
        
        return {
            "all": all_models,
            "local": local_models,
            "cloud": cloud_models,
            "default_model": self.default_model,
            "total_available": len([m for m in all_models if m["status"] == "ready"])
        }
    
    async def get_default_model(self) -> str:
        """Obtener modelo por defecto"""
        return self.default_model
    
    async def set_api_key(self, model_id: str, api_key: str) -> bool:
        """Configurar API key para modelo cloud"""
        if model_id not in self.model_configs:
            return False
        
        config = self.model_configs[model_id]
        if not config.get("requires_api_key"):
            return False
        
        # Validar formato
        if self._validate_api_key(model_id, api_key):
            self.api_keys[model_id] = api_key
            config["status"] = ModelStatus.READY
            logger.info(f"‚úÖ API key configurada para {model_id}")
            return True
        else:
            config["status"] = ModelStatus.ERROR
            logger.error(f"‚ùå API key inv√°lida para {model_id}")
            return False
    
    def _validate_api_key(self, model_id: str, api_key: str) -> bool:
        """Validar formato de API key"""
        validations = {
            "gpt4": api_key.startswith("sk-") and len(api_key) > 45,
            "claude": api_key.startswith("sk-ant-") and len(api_key) > 60,
            "gemini": len(api_key) > 30
        }
        return validations.get(model_id, True)
    
    async def chat(self, message: str, model_id: Optional[str] = None, conversation_history: List[Dict] = None) -> Dict[str, Any]:
        """Chat con modelo especificado"""
        
        self.stats["total_requests"] += 1
        self.stats["last_request"] = datetime.now().isoformat()
        
        # Usar modelo por defecto si no se especifica
        if not model_id:
            model_id = self.default_model
        
        # Verificar disponibilidad
        if model_id not in self.model_configs:
            self.stats["failed_requests"] += 1
            return {"error": f"Modelo {model_id} no reconocido"}
        
        if self.model_configs[model_id]["status"] != ModelStatus.READY:
            self.stats["failed_requests"] += 1
            return {"error": f"Modelo {model_id} no disponible"}
        
        try:
            # Enrutar seg√∫n modelo
            if model_id == "bert":
                result = await self._chat_bert(message, conversation_history)
            elif model_id == "llama":
                result = await self._chat_ollama(message, "llama3.1", conversation_history)
            elif model_id == "r1":
                result = await self._chat_ollama(message, "deepseek-r1", conversation_history)
            elif model_id == "gpt4":
                result = await self._chat_openai(message, conversation_history)
            elif model_id == "claude":
                result = await self._chat_anthropic(message, conversation_history)
            elif model_id == "gemini":
                result = await self._chat_google(message, conversation_history)
            else:
                result = {"error": f"Modelo {model_id} no implementado"}
            
            if "error" not in result:
                self.stats["successful_requests"] += 1
            else:
                self.stats["failed_requests"] += 1
            
            return result
            
        except Exception as e:
            self.stats["failed_requests"] += 1
            logger.error(f"Error en chat con {model_id}: {e}")
            return {"error": f"Error interno: {str(e)}"}
    
    async def _chat_bert(self, message: str, history: List[Dict] = None) -> Dict[str, Any]:
        """Chat con BERT"""
        try:
            chatbot = self.loaded_models["bert"]
            conversation = Conversation(message)
            
            result = chatbot(conversation)
            response_text = result.generated_responses[-1] if result.generated_responses else "Sin respuesta disponible"
            
            return {
                "response": response_text,
                "model": "bert",
                "provider": "Hugging Face",
                "timestamp": datetime.now().isoformat(),
                "tokens_used": len(message.split()) + len(response_text.split())
            }
            
        except Exception as e:
            return {"error": f"Error con BERT: {str(e)}"}
    
    async def _chat_ollama(self, message: str, model_name: str, history: List[Dict] = None) -> Dict[str, Any]:
        """Chat con Ollama"""
        if not HTTPX_AVAILABLE:
            return {"error": "httpx no disponible para Ollama"}
        
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    "http://localhost:11434/api/generate",
                    json={
                        "model": model_name,
                        "prompt": message,
                        "stream": False,
                        "options": {
                            "temperature": 0.7,
                            "top_p": 0.9,
                            "max_tokens": 500
                        }
                    }
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return {
                        "response": result.get("response", "Sin respuesta"),
                        "model": model_name,
                        "provider": "Ollama",
                        "timestamp": datetime.now().isoformat(),
                        "tokens_used": result.get("eval_count", 0)
                    }
                else:
                    return {"error": f"Error Ollama HTTP {response.status_code}"}
                    
        except Exception as e:
            return {"error": f"Error Ollama: {str(e)}"}
    
    async def _chat_openai(self, message: str, history: List[Dict] = None) -> Dict[str, Any]:
        """Chat con OpenAI GPT-4"""
        if not OPENAI_AVAILABLE:
            return {"error": "OpenAI no disponible"}
        
        api_key = self.api_keys.get("gpt4")
        if not api_key:
            return {"error": "API key de OpenAI requerida"}
        
        try:
            client = openai.AsyncOpenAI(api_key=api_key)
            
            messages = [{"role": "system", "content": "Eres un asistente IA √∫til."}]
            messages.append({"role": "user", "content": message})
            
            response = await client.chat.completions.create(
                model="gpt-4",
                messages=messages,
                max_tokens=500,
                temperature=0.7
            )
            
            return {
                "response": response.choices[0].message.content,
                "model": "gpt-4",
                "provider": "OpenAI",
                "timestamp": datetime.now().isoformat(),
                "tokens_used": response.usage.total_tokens if response.usage else 0
            }
            
        except Exception as e:
            return {"error": f"Error GPT-4: {str(e)}"}
    
    async def _chat_anthropic(self, message: str, history: List[Dict] = None) -> Dict[str, Any]:
        """Chat con Claude"""
        if not ANTHROPIC_AVAILABLE:
            return {"error": "Anthropic no disponible"}
        
        api_key = self.api_keys.get("claude")
        if not api_key:
            return {"error": "API key de Anthropic requerida"}
        
        try:
            client = anthropic.AsyncAnthropic(api_key=api_key)
            
            response = await client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=500,
                messages=[{"role": "user", "content": message}]
            )
            
            return {
                "response": response.content[0].text,
                "model": "claude-3.5-sonnet",
                "provider": "Anthropic",
                "timestamp": datetime.now().isoformat(),
                "tokens_used": response.usage.input_tokens + response.usage.output_tokens
            }
            
        except Exception as e:
            return {"error": f"Error Claude: {str(e)}"}
    
    async def _chat_google(self, message: str, history: List[Dict] = None) -> Dict[str, Any]:
        """Chat con Gemini"""
        if not GOOGLE_AI_AVAILABLE:
            return {"error": "Google AI no disponible"}
        
        api_key = self.api_keys.get("gemini")
        if not api_key:
            return {"error": "API key de Google requerida"}
        
        try:
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-pro')
            
            response = model.generate_content(message)
            
            return {
                "response": response.text,
                "model": "gemini-pro",
                "provider": "Google",
                "timestamp": datetime.now().isoformat(),
                "tokens_used": len(message.split()) + len(response.text.split())
            }
            
        except Exception as e:
            return {"error": f"Error Gemini: {str(e)}"}
    
    async def get_stats(self) -> Dict[str, Any]:
        """Obtener estad√≠sticas del motor"""
        return {
            **self.stats,
            "models_available": len([c for c in self.model_configs.values() if c["status"] == ModelStatus.READY]),
            "models_configured": len(self.model_configs),
            "uptime": "N/A"  # Se puede implementar despu√©s
        }
    
    async def cleanup(self):
        """Limpiar recursos"""
        logger.info("üßπ Limpiando HybridLLMEngine...")
        
        # Limpiar modelos cargados
        for model_name in list(self.loaded_models.keys()):
            del self.loaded_models[model_name]
        
        self.loaded_models.clear()
        self.is_initialized = False
        
        logger.info("‚úÖ Recursos limpiados")
'''
        
        with open(self.ai_engine_path / "models" / "llm_hybrid_engine.py", "w", encoding="utf-8") as f:
            f.write(hybrid_engine_content)
        
        print("‚úÖ Hybrid Engine actualizado")
    
    def update_routers(self):
        """Actualizar todos los routers"""
        print("üåê Actualizando routers...")
        
        # Router de chat
        chat_router_content = '''from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel
from typing import List, Dict, Optional, Any
import logging

router = APIRouter()
logger = logging.getLogger(__name__)

class ChatRequest(BaseModel):
    message: str
    model: Optional[str] = None
    history: Optional[List[Dict]] = []

class ChatResponse(BaseModel):
    response: str
    model: str
    provider: str
    timestamp: str
    tokens_used: Optional[int] = 0

@router.post("/", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    """Endpoint de chat principal"""
    try:
        # Este endpoint es manejado por app.py principal
        # Solo documentaci√≥n aqu√≠
        return {
            "response": "Este endpoint se maneja en app.py principal",
            "model": request.model or "unknown",
            "provider": "ai-engine",
            "timestamp": "2024-01-01T00:00:00",
            "tokens_used": 0
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/history")
async def get_chat_history():
    """Obtener historial de chat"""
    return {"history": [], "total": 0}
'''
        
        # Router de models
        models_router_content = '''from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Optional
import logging

router = APIRouter()
logger = logging.getLogger(__name__)

class ModelInfo(BaseModel):
    id: str
    name: str
    type: str
    status: str
    description: str
    icon: str
    provider: str
    requires_api_key: bool

@router.get("/", response_model=List[ModelInfo])
async def list_models():
    """Listar modelos disponibles"""
    # Este endpoint es manejado por app.py principal
    return []

@router.get("/default")
async def get_default_model():
    """Obtener modelo por defecto"""
    return {"default_model": "bert"}

@router.post("/configure")
async def configure_model(request: dict):
    """Configurar modelo"""
    return {"status": "configured"}
'''
        
        # Router de analysis
        analysis_router_content = '''from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Dict, Any
import logging

router = APIRouter()
logger = logging.getLogger(__name__)

class AnalysisRequest(BaseModel):
    text: str
    analysis_type: str = "sentiment"

@router.post("/text")
async def analyze_text(request: AnalysisRequest):
    """An√°lisis de texto"""
    try:
        return {
            "analysis": {
                "sentiment": {"score": 0.7, "label": "positive"},
                "entities": [],
                "keywords": []
            },
            "text_length": len(request.text),
            "analysis_type": request.analysis_type
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
'''
        
        # Router de generation
        generation_router_content = '''from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Optional
import logging

router = APIRouter()
logger = logging.getLogger(__name__)

class GenerationRequest(BaseModel):
    prompt: str
    model: Optional[str] = None
    max_tokens: Optional[int] = 500

@router.post("/text")
async def generate_text(request: GenerationRequest):
    """Generar texto"""
    try:
        return {
            "generated_text": f"Texto generado basado en: {request.prompt[:50]}...",
            "model": request.model or "default",
            "tokens_generated": 25
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
'''
        
        # Router de training
        training_router_content = '''from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Dict, Any
import logging

router = APIRouter()
logger = logging.getLogger(__name__)

class TrainingRequest(BaseModel):
    model_name: str
    dataset_path: str
    config: Dict[str, Any] = {}

@router.post("/start")
async def start_training(request: TrainingRequest):
    """Iniciar entrenamiento"""
    try:
        return {
            "training_id": "train_001",
            "status": "started",
            "model_name": request.model_name,
            "estimated_time": "2 hours"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/status/{training_id}")
async def get_training_status(training_id: str):
    """Estado del entrenamiento"""
    return {
        "training_id": training_id,
        "status": "running",
        "progress": 0.45,
        "eta": "1 hour"
    }
'''
        
        # Escribir routers
        routers = {
            "chat.py": chat_router_content,
            "models.py": models_router_content,
            "analysis.py": analysis_router_content,
            "generation.py": generation_router_content,
            "training.py": training_router_content
        }
        
        for filename, content in routers.items():
            with open(self.ai_engine_path / "routers" / filename, "w", encoding="utf-8") as f:
                f.write(content)
        
        print("‚úÖ Routers actualizados")
    
    def update_model_manager(self):
        """Actualizar model manager"""
        print("‚öôÔ∏è Actualizando Model Manager...")
        
        model_manager_content = '''import asyncio
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime

logger = logging.getLogger(__name__)

class ModelManager:
    """Gestor de modelos centralizado"""
    
    def __init__(self):
        self.loaded_models = {}
        self.model_stats = {}
        self.is_initialized = False
    
    async def initialize(self):
        """Inicializar el gestor"""
        logger.info("‚öôÔ∏è Inicializando ModelManager...")
        self.is_initialized = True
        logger.info("‚úÖ ModelManager inicializado")
    
    def get_local_models(self) -> List[str]:
        """Obtener modelos locales"""
        return list(self.loaded_models.keys())
    
    def get_cloud_models(self) -> List[str]:
        """Obtener modelos cloud"""
        return ["gpt4", "claude", "gemini"]
    
    def get_memory_usage(self) -> Dict[str, Any]:
        """Obtener uso de memoria"""
        return {
            "total_mb": 0,
            "models": {},
            "available_mb": 1000
        }
    
    async def load_model(self, model_name: str) -> bool:
        """Cargar modelo espec√≠fico"""
        try:
            logger.info(f"üì• Cargando modelo: {model_name}")
            # Implementaci√≥n de carga aqu√≠
            self.loaded_models[model_name] = f"model_{model_name}"
            return True
        except Exception as e:
            logger.error(f"‚ùå Error cargando {model_name}: {e}")
            return False
    
    async def unload_model(self, model_name: str) -> bool:
        """Descargar modelo"""
        try:
            if model_name in self.loaded_models:
                del self.loaded_models[model_name]
                logger.info(f"üì§ Modelo descargado: {model_name}")
                return True
            return False
        except Exception as e:
            logger.error(f"‚ùå Error descargando {model_name}: {e}")
            return False
    
    def is_model_loaded(self, model_name: str) -> bool:
        """Verificar si modelo est√° cargado"""
        return model_name in self.loaded_models
    
    async def preload_default_models(self):
        """Precargar modelos por defecto"""
        logger.info("üöÄ Precargando modelos por defecto...")
        default_models = ["bert"]
        
        for model_name in default_models:
            await self.load_model(model_name)
        
        logger.info("‚úÖ Modelos por defecto precargados")
'''
        
        with open(self.ai_engine_path / "services" / "model_manager.py", "w", encoding="utf-8") as f:
            f.write(model_manager_content)
        
        print("‚úÖ Model Manager actualizado")
    
    def create_config_files(self):
        """Crear archivos de configuraci√≥n"""
        print("üîß Creando archivos de configuraci√≥n...")
        
        # config/__init__.py
        (self.ai_engine_path / "config" / "__init__.py").touch()
        
        # utils/__init__.py y archivos b√°sicos
        (self.ai_engine_path / "utils" / "__init__.py").touch()
        
        # utils/config.py
        config_content = '''import os
from typing import List

class Settings:
    """Configuraci√≥n del AI Engine"""
    
    def __init__(self):
        self.debug = os.getenv("DEBUG", "false").lower() == "true"
        self.cors_origins = ["*"]
        self.preload_models = os.getenv("PRELOAD_MODELS", "true").lower() == "true"
        
        # API Keys
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.anthropic_api_key = os.getenv("ANTHROPIC_API_KEY")
        self.google_ai_api_key = os.getenv("GOOGLE_AI_API_KEY")
        
        # Ollama
        self.ollama_host = os.getenv("OLLAMA_HOST", "localhost")
        self.ollama_port = int(os.getenv("OLLAMA_PORT", "11434"))
'''
        
        # utils/logging_config.py
        logging_config_content = '''import logging
import os
from pathlib import Path

def setup_logging():
    """Configurar logging"""
    
    # Crear directorio de logs si no existe
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    # Configurar logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler("logs/ai_engine.log"),
            logging.StreamHandler()
        ]
    )
    
    # Configurar loggers espec√≠ficos
    logging.getLogger("transformers").setLevel(logging.WARNING)
    logging.getLogger("httpx").setLevel(logging.WARNING)
'''
        
        # utils/auth.py
        auth_content = '''from fastapi import HTTPException, Depends
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials

security = HTTPBearer(auto_error=False)

async def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """Verificar token de autenticaci√≥n"""
    # Por ahora, permitir acceso sin autenticaci√≥n
    # En producci√≥n, implementar verificaci√≥n real
    return True
'''
        
        # Escribir archivos
        with open(self.ai_engine_path / "utils" / "config.py", "w", encoding="utf-8") as f:
            f.write(config_content)
        
        with open(self.ai_engine_path / "utils" / "logging_config.py", "w", encoding="utf-8") as f:
            f.write(logging_config_content)
        
        with open(self.ai_engine_path / "utils" / "auth.py", "w", encoding="utf-8") as f:
            f.write(auth_content)
        
        print("‚úÖ Archivos de configuraci√≥n creados")
    
    def update_requirements(self):
        """Actualizar requirements.txt"""
        print("üì¶ Actualizando requirements.txt...")
        
        requirements_content = '''# ==================================================
# AI ENGINE v6.0 - REQUIREMENTS
# ==================================================

# FastAPI y servidor
fastapi==0.104.1
uvicorn==0.24.0
python-multipart==0.0.6

# HTTP y requests
httpx==0.25.0
requests==2.31.0

# Machine Learning y IA
torch>=2.0.0
transformers>=4.35.0
tokenizers>=0.14.0

# APIs de IA (opcionales)
openai>=1.3.0
anthropic>=0.7.0
google-generativeai>=0.3.0

# Base de datos y storage
sqlalchemy==2.0.23
sqlite3

# Utilidades
python-dotenv==1.0.0
pydantic==2.5.0
pydantic-settings==2.1.0

# Logging y monitoreo
python-json-logger==2.0.7

# Testing (opcional)
pytest==7.4.3
pytest-asyncio==0.21.1

# Desarrollo (opcional)
black==23.11.0
flake8==6.1.0
'''
        
        with open(self.ai_engine_path / "requirements.txt", "w", encoding="utf-8") as f:
            f.write(requirements_content)
        
        print("‚úÖ requirements.txt actualizado")
    
    def verify_functionality(self):
        """Verificar funcionalidad b√°sica"""
        print("üß™ Verificando funcionalidad...")
        
        # Verificar que todos los archivos existen
        required_files = [
            "app.py",
            "models/llm_hybrid_engine.py",
            "services/model_manager.py",
            "routers/chat.py",
            "routers/models.py",
            "utils/config.py",
            "requirements.txt"
        ]
        
        missing_files = []
        for file_path in required_files:
            if not (self.ai_engine_path / file_path).exists():
                missing_files.append(file_path)
        
        if missing_files:
            print(f"‚ö†Ô∏è Archivos faltantes: {missing_files}")
        else:
            print("‚úÖ Todos los archivos requeridos existen")
        
        # Crear script de prueba
        test_script_content = '''#!/usr/bin/env python3
"""
Script de prueba r√°pida del AI Engine
"""

import sys
import asyncio
from pathlib import Path

# Agregar path del proyecto
sys.path.append(str(Path(__file__).parent))

async def test_basic_functionality():
    """Probar funcionalidad b√°sica"""
    try:
        from models.llm_hybrid_engine import HybridLLMEngine
        
        print("üß™ Probando HybridLLMEngine...")
        engine = HybridLLMEngine()
        await engine.initialize()
        
        models = await engine.get_available_models()
        print(f"‚úÖ Modelos disponibles: {len(models.get('all', []))}")
        
        if models.get('all'):
            # Probar chat con primer modelo disponible
            first_model = models['all'][0]
            if first_model['status'] == 'ready':
                response = await engine.chat("Hola, ¬øfuncionas?", first_model['id'])
                print(f"‚úÖ Chat test: {response.get('response', 'Error')[:50]}...")
        
        await engine.cleanup()
        print("‚úÖ Test b√°sico completado")
        
    except Exception as e:
        print(f"‚ùå Error en test: {e}")

if __name__ == "__main__":
    asyncio.run(test_basic_functionality())
'''
        
        with open(self.ai_engine_path / "test_functionality.py", "w", encoding="utf-8") as f:
            f.write(test_script_content)
        
        print("‚úÖ Script de prueba creado")

# =====================
# MAIN EXECUTION
# =====================

if __name__ == "__main__":
    setup = AIEngineCompleteSetup()
    
    if setup.run():
        print("\nüéâ ¬°AI ENGINE COMPLETADO EXITOSAMENTE!")
        print("\nüìã PR√ìXIMOS PASOS:")
        print("1. cd services/ai-engine")
        print("2. pip install -r requirements.txt")
        print("3. python app.py")
        print("4. Verificar: http://localhost:8001/health")
        print("5. Docs: http://localhost:8001/docs")
        
        print("\nüîß ENDPOINTS PRINCIPALES:")
        print("- GET  /models - Obtener modelos disponibles")
        print("- POST /chat - Chat con IA")
        print("- POST /configure - Configurar API keys")
        print("- POST /test-model - Probar modelo espec√≠fico")
        
    else:
        print("\n‚ùå Error en el setup. Revisa los logs.")
        sys.exit(1)